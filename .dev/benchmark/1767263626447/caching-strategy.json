{
  "caseId": "caching-strategy",
  "caseName": "복잡한 캐싱 무효화 전략",
  "caseType": "tech",
  "category": "performance",
  "controversyLevel": "medium",
  "singleAI": {
    "response": "# 소셜 미디어 피드 캐싱 전략 분석\n\n## 결론: **B + C 하이브리드 전략**\n\n단일 전략으로는 요구사항을 충족하기 어렵습니다. 게시물 단위 캐싱과 Write-through를 조합한 하이브리드 접근이 필요합니다.\n\n---\n\n## 각 선택지 분석\n\n### A) 사용자별 피드 캐싱 ❌ 단독 사용 부적합\n\n```\n문제점:\n- 100만 DAU × 피드당 ~50KB = 50GB+ 메모리 필요\n- 게시물 수정 시 팔로워 전원 캐시 무효화 = 셀럽의 경우 100만건 삭제\n- 좋아요 수 변경마다 수만 개 캐시 갱신 불가능\n```\n\n### B) 게시물 단위 캐싱 ✅ 핵심 전략\n\n```\n장점:\n- 게시물 1개 수정 = 캐시 1개만 무효화\n- 메모리 효율적 (중복 저장 없음)\n- 좋아요 수 업데이트가 단일 키에서 처리됨\n\n단점:\n- 피드 조회 시 N개 게시물 조합 필요 → 해결책 필요\n```\n\n### C) Write-through ✅ 실시간성 확보\n\n```\n장점:\n- 쓰기 시점에 캐시 즉시 갱신\n- 좋아요/댓글 실시간 반영 가능\n\n단점:\n- 쓰기 성능 저하 → 비동기 처리로 해결\n```\n\n### D) Read-through + TTL ⚠️ 보조 전략으로만\n\n```\n장점:\n- 구현 단순\n\n단점:\n- TTL 만료 전까지 stale 데이터\n- 실시간성 요구사항 불충족\n```\n\n---\n\n## 권장 아키텍처\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                         Feed Request                            │\n└─────────────────────────────────────────────────────────────────┘\n                                │\n                                ▼\n┌─────────────────────────────────────────────────────────────────┐\n│  Layer 1: Feed ID List Cache (Redis Sorted Set)                 │\n│  ─────────────────────────────────────────────────────────────  │\n│  Key: feed:{user_id}                                            │\n│  Value: [post_id1, post_id2, ...] (최근 100개)                   │\n│  TTL: 5분 (stale해도 무방한 데이터)                               │\n└─────────────────────────────────────────────────────────────────┘\n                                │\n                                ▼\n┌─────────────────────────────────────────────────────────────────┐\n│  Layer 2: Post Content Cache (Redis Hash)                       │\n│  ─────────────────────────────────────────────────────────────  │\n│  Key: post:{post_id}                                            │\n│  Fields: content, author_id, created_at, image_url              │\n│  TTL: 1시간 (수정 시 즉시 무효화)                                 │\n└─────────────────────────────────────────────────────────────────┘\n                                │\n                                ▼\n┌─────────────────────────────────────────────────────────────────┐\n│  Layer 3: Counter Cache (Redis String + INCR)                   │\n│  ─────────────────────────────────────────────────────────────  │\n│  Key: count:{post_id}:likes / count:{post_id}:comments          │\n│  TTL: 없음 (Write-through로 항상 최신)                           │\n└─────────────────────────────────────────────────────────────────┘\n                                │\n                                ▼\n┌─────────────────────────────────────────────────────────────────┐\n│  Layer 4: Author Profile Cache (Redis Hash)                     │\n│  ─────────────────────────────────────────────────────────────  │\n│  Key: user:{user_id}                                            │\n│  Fields: name, avatar_url, verified                             │\n│  TTL: 10분                                                       │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n---\n\n## 핵심 구현\n\n### 1. 피드 조회 (목표: 50ms 이내)\n\n```python\nasync def get_feed(user_id: str, limit: int = 20) -> List[Post]:\n    # Step 1: 피드 ID 목록 조회 (Cache-Aside)\n    post_ids = await redis.zrevrange(f\"feed:{user_id}\", 0, limit - 1)\n    \n    if not post_ids:\n        # Cache miss: DB에서 조회 후 캐싱\n        post_ids = await db.get_feed_post_ids(user_id, limit)\n        await redis.zadd(f\"feed:{user_id}\", {pid: ts for pid, ts in post_ids})\n        await redis.expire(f\"feed:{user_id}\", 300)  # 5분 TTL\n    \n    # Step 2: 병렬로 모든 데이터 조회 (MGET 사용)\n    posts, counters, authors = await asyncio.gather(\n        redis.mget([f\"post:{pid}\" for pid in post_ids]),\n        redis.mget([f\"count:{pid}\" for pid in post_ids]),\n        get_unique_authors(post_ids)  # 중복 제거 후 조회\n    )\n    \n    # Step 3: 조합하여 반환\n    return assemble_feed(posts, counters, authors)\n```\n\n### 2. 좋아요 처리 (Write-through)\n\n```python\nasync def like_post(post_id: str, user_id: str):\n    # DB 업데이트와 캐시 업데이트를 트랜잭션으로\n    async with db.transaction():\n        await db.insert_like(post_id, user_id)\n        # Redis INCR은 atomic - 동시성 안전\n        await redis.incr(f\"count:{post_id}:likes\")\n```\n\n### 3. 게시물 수정 시 무효화\n\n```python\nasync def update_post(post_id: str, content: str):\n    await db.update_post(post_id, content)\n    \n    # 게시물 캐시만 삭제 (피드 목록은 그대로)\n    await redis.delete(f\"post:{post_id}\")\n    \n    # 팔로워 피드는 건드리지 않음!\n    # → 다음 조회 시 자동으로 새 데이터 로드\n```\n\n---\n\n## 왜 팔로워 전원의 캐시 무효화가 불필요한가?\n\n```\n기존 우려: 게시물 수정 → 팔로워 100만명 캐시 무효화?\n\n해결:\n┌──────────────────────────────────────────────────────────┐\n│  feed:{user_id} 캐시에는 post_id 목록만 저장              │\n│  실제 게시물 내용은 post:{post_id}에 별도 저장            │\n│                                                          │\n│  게시물 수정 시:                                          │\n│  - post:{post_id} 1개만 삭제하면 됨                       │\n│  - 모든 사용자가 다음 조회 시 새 내용 자동 로드           │\n└──────────────────────────────────────────────────────────┘\n```\n\n---\n\n## 성능 예측\n\n| 단계 | 시간 | 설명 |\n|------|------|------|\n| 피드 ID 조회 | 1-2ms | Redis ZRANGE |\n| 게시물 20개 조회 | 2-3ms | Redis MGET (병렬) |\n| 카운터 조회 | 1-2ms | Redis MGET (병렬) |\n| 프로필 조회 | 2-3ms | Redis MGET (중복 제거) |\n| 네트워크 오버헤드 | 5-10ms | 클라이언트 ↔ Redis |\n| **총합** | **~20ms** | 목표 50ms 충족 |\n\n---\n\n## 추가 고려사항\n\n### 핫 게시물 처리\n\n```python\n# 좋아요가 초당 수천 건인 경우 → 버퍼링\nasync def like_post_buffered(post_id: str):\n    # Local buffer에 누적\n    await redis.incr(f\"buffer:likes:{post_id}\")\n    \n# 1초마다 배치로 DB 반영\n@scheduler.every(1, 'seconds')\nasync def flush_like_buffer():\n    for key in await redis.scan(\"buffer:likes:*\"):\n        count = await redis.getdel(key)\n        post_id = key.split(\":\")[-1]\n        await db.increment_likes(post_id, count)\n```\n\n### 캐시 워밍\n\n```python\n# 신규 게시물 작성 시 → 팔로워 피드에 push\nasync def create_post(author_id: str, content: str):\n    post_id = await db.create_post(author_id, content)\n    \n    # 즉시 캐시에 저장\n    await redis.hset(f\"post:{post_id}\", mapping={...})\n    await redis.set(f\"count:{post_id}:likes\", 0)\n    \n    # 활성 팔로워의 피드에만 push (최근 1시간 접속)\n    active_followers = await get_active_followers(author_id)\n    for follower_id in active_followers:\n        await redis.zadd(f\"feed:{follower_id}\", {post_id: time.time()})\n```\n\n---\n\n## 요약\n\n| 데이터 | 캐싱 전략 | 무효화 방식 | TTL |\n|--------|----------|------------|-----|\n| 피드 목록 | Cache-Aside | 새 게시물 시 Push | 5분 |\n| 게시물 내용 | Cache-Aside | 수정 시 Delete | 1시간 |\n| 좋아요/댓글 수 | Write-through | 없음 (항상 최신) | 없음 |\n| 프로필 | Cache-Aside | 수정 시 Delete | 10분 |\n\n**핵심 원칙**: 데이터를 분리하여 각각의 변경 주기에 맞는 전략을 적용하면 복잡한 무효화 문제를 피할 수 있습니다.",
    "timeMs": 60411
  },
  "multiParallel": {
    "responses": [
      {
        "ai": "claude",
        "content": "# 소셜 미디어 피드 캐싱 전략 분석\n\n## 핵심 문제 분석\n\n현재 상황을 정리하면:\n\n| 항목 | 현재 | 목표 | 난이도 |\n|------|------|------|--------|\n| 응답 시간 | 500ms | 50ms | **10배 개선 필요** |\n| 요청량 | 5,000 RPS | - | Redis 충분히 처리 가능 |\n| 실시간성 | 좋아요/댓글 즉시 반영 | - | 캐시 무효화 복잡 |\n\n---\n\n## 각 선택지 분석\n\n### A) 사용자별 피드 캐싱\n\n```\nUser A의 피드 → Redis에 완성된 피드 저장\n```\n\n**문제점:**\n- 팔로워 1000명인 유저가 글 수정 → **1000개 캐시 무효화**\n- 메모리: 100만 DAU × 평균 50개 게시물 = **엄청난 중복 저장**\n- 좋아요 1개 → 해당 게시물 본 모든 유저 캐시 무효화?\n\n**결론: ❌ 규모가 커질수록 파탄**\n\n---\n\n### B) 게시물 단위 캐싱 + 실시간 조합\n\n```\n피드 요청 → 팔로우 목록 조회 → 각 게시물 캐시에서 조합\n```\n\n**문제점:**\n- 피드 1회 요청 = Redis 호출 20~50회 (게시물 수만큼)\n- 네트워크 왕복 시간만 해도 50ms 초과 가능\n- 정렬/필터링 로직이 애플리케이션으로 이동\n\n**결론: ⚠️ 단독으로는 부족**\n\n---\n\n### C) Write-through + 백그라운드 갱신\n\n```\n글 작성 → DB 저장 + 팔로워 피드 캐시에 push\n```\n\n**문제점:**\n- 인플루언서(팔로워 100만) 글 작성 → **100만 번 캐시 업데이트**\n- Write 부하가 극심\n- 팔로워 대부분은 피드를 안 볼 수도 있음 (낭비)\n\n**결론: ❌ Fan-out on write는 대규모에서 비효율**\n\n---\n\n### D) Read-through + TTL 기반\n\n```\n피드 요청 → 캐시 있으면 반환, 없으면 DB 조회 후 캐싱\n```\n\n**문제점:**\n- TTL 1분이면 → 좋아요가 1분간 반영 안됨\n- TTL 1초면 → 캐시 효과 없음\n- 실시간성 요구사항과 충돌\n\n**결론: ⚠️ 단독으로는 실시간성 불충족**\n\n---\n\n## 권장 전략: 하이브리드 계층형 캐싱\n\n단일 전략으로는 불가능합니다. **B + D + 분리 전략**을 조합합니다:\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                        피드 요청                                  │\n└─────────────────────────────────────────────────────────────────┘\n                              │\n                              ▼\n┌─────────────────────────────────────────────────────────────────┐\n│  Layer 1: 피드 구조 캐시 (TTL 5분)                                │\n│  ┌─────────────────────────────────────────────────────────────┐│\n│  │ feed:user:123 = [post_id_1, post_id_2, post_id_3, ...]     ││\n│  │ (게시물 ID 목록만 저장, 내용 없음)                             ││\n│  └─────────────────────────────────────────────────────────────┘│\n└─────────────────────────────────────────────────────────────────┘\n                              │\n                              ▼\n┌─────────────────────────────────────────────────────────────────┐\n│  Layer 2: 게시물 정적 데이터 (TTL 1시간)                          │\n│  ┌─────────────────────────────────────────────────────────────┐│\n│  │ post:456 = {                                                ││\n│  │   content: \"...\",                                           ││\n│  │   author_id: 789,                                           ││\n│  │   created_at: \"...\",                                        ││\n│  │   image_url: \"...\"                                          ││\n│  │ }                                                           ││\n│  └─────────────────────────────────────────────────────────────┘│\n└─────────────────────────────────────────────────────────────────┘\n                              │\n                              ▼\n┌─────────────────────────────────────────────────────────────────┐\n│  Layer 3: 카운터 분리 (실시간)                                    │\n│  ┌─────────────────────────────────────────────────────────────┐│\n│  │ counter:post:456:likes = 1234      ← Redis INCR/DECR       ││\n│  │ counter:post:456:comments = 56                              ││\n│  └─────────────────────────────────────────────────────────────┘│\n└─────────────────────────────────────────────────────────────────┘\n                              │\n                              ▼\n┌─────────────────────────────────────────────────────────────────┐\n│  Layer 4: 작성자 프로필 (TTL 10분)                                │\n│  ┌─────────────────────────────────────────────────────────────┐│\n│  │ profile:789 = {name: \"...\", avatar: \"...\"}                  ││\n│  └─────────────────────────────────────────────────────────────┘│\n└─────────────────────────────────────────────────────────────────┘\n```\n\n---\n\n## 구현 핵심\n\n### 1. 피드 조회 흐름 (Pipeline 사용)\n\n```python\nasync def get_feed(user_id: str) -> List[Post]:\n    # 1. 피드 구조 가져오기 (or 생성)\n    post_ids = await redis.get(f\"feed:user:{user_id}\")\n    if not post_ids:\n        post_ids = await build_feed_from_db(user_id)\n        await redis.setex(f\"feed:user:{user_id}\", 300, post_ids)  # 5분\n    \n    # 2. Pipeline으로 모든 데이터 한 번에 조회\n    pipe = redis.pipeline()\n    for pid in post_ids[:20]:  # 페이지네이션\n        pipe.get(f\"post:{pid}\")           # 정적 데이터\n        pipe.get(f\"counter:post:{pid}:likes\")\n        pipe.get(f\"counter:post:{pid}:comments\")\n    \n    results = await pipe.execute()  # 1회 네트워크 왕복\n    \n    # 3. 조합 및 반환\n    return assemble_feed(results)\n```\n\n**Pipeline 효과: Redis 호출 60회 → 1회**\n\n### 2. 좋아요 처리 (실시간)\n\n```python\nasync def like_post(post_id: str, user_id: str):\n    # DB 저장 (비동기)\n    await db.likes.insert(post_id, user_id)\n    \n    # 카운터만 즉시 증가 (다른 캐시 무효화 불필요!)\n    await redis.incr(f\"counter:post:{post_id}:likes\")\n```\n\n**핵심: 카운터 분리로 캐시 무효화 문제 해결**\n\n### 3. 게시물 수정 시\n\n```python\nasync def update_post(post_id: str, content: str):\n    await db.posts.update(post_id, content)\n    \n    # 해당 게시물 캐시만 삭제 (팔로워 캐시는 건드리지 않음!)\n    await redis.delete(f\"post:{post_id}\")\n```\n\n**팔로워 피드 캐시는 다음 조회 시 자동으로 최신 데이터 fetch**\n\n---\n\n## 무효화 전략 정리\n\n| 이벤트 | 무효화 대상 | 영향 범위 |\n|--------|-------------|-----------|\n| 좋아요/댓글 | 없음 (카운터 INCR) | O(1) |\n| 게시물 수정 | `post:{id}` | O(1) |\n| 게시물 삭제 | `post:{id}` + 피드 재구성 트리거 | O(1) |\n| 프로필 수정 | `profile:{id}` | O(1) |\n| 새 게시물 | 없음 (피드 TTL 만료 시 반영) | O(1) |\n| 팔로우/언팔 | `feed:user:{id}` | O(1) |\n\n---\n\n## 성능 예측\n\n```\n기존: DB 쿼리 (JOIN 다수) = 500ms\n\n개선 후:\n  - Redis Pipeline 1회: ~2ms\n  - 캐시 미스 시 DB 조회: ~50ms (드묾)\n  - 평균: ~5-10ms\n```\n\n**목표 50ms 달성 가능** (여유 있음)\n\n---\n\n## 추가 고려사항\n\n### 콜드 스타트 방지\n```python\n# 신규 가입/오랜만에 접속 유저\nif not await redis.exists(f\"feed:user:{user_id}\"):\n    # 백그라운드로 미리 빌드\n    await queue.enqueue(\"build_feed\", user_id)\n    # 즉시 DB에서 조회해서 반환\n    return await fetch_feed_from_db(user_id)\n```\n\n### 카운터 DB 동기화\n```python\n# 1분마다 배치로 DB 동기화 (데이터 정합성)\n@scheduled(every=\"1m\")\nasync def sync_counters():\n    keys = await redis.keys(\"counter:post:*:likes\")\n    for key in keys:\n        post_id = extract_post_id(key)\n        count = await redis.get(key)\n        await db.posts.update(post_id, likes_count=count)\n```\n\n---\n\n## 결론\n\n**단일 전략(A/B/C/D)으로는 불가능합니다.**\n\n권장 조합:\n- **B (게시물 단위)** + **D (Read-through/TTL)** 기반\n- **카운터 분리**로 실시간성 확보\n- **Pipeline**으로 네트워크 오버헤드 제거\n- **계층별 TTL 차등화**로 캐시 효율 극대화\n\n이 구조는 Pinterest, Instagram 등 대규모 서비스에서 검증된 패턴입니다."
      },
      {
        "ai": "codex",
        "content": "요구사항 기준으로는 단일 전략보다 “하이브리드 + 계층 캐시 + 이벤트 기반 무효화”가 가장 안정적입니다. 아래처럼 설계하는 걸 추천합니다.\n\n권장 전략\n- 기본: B(게시물 단위 캐싱 + 실시간 조합) + 부분적으로 A\n- 이유: 사용자별 피드 전체 캐시는 무효화 비용이 크고, 좋아요/댓글처럼 고빈도 업데이트에 취약. 대신 게시물/프로필/카운트를 분리 캐싱하고, 피드 엔트리는 짧은 TTL 또는 세션 캐시로만 사용.\n\n캐시 구조\n- Feed Index (per user): 최근 N개 게시물 ID 리스트\n  - Redis ZSET/List, TTL 30–120s\n  - 업데이트는 팔로우/언팔, 새 पोस्ट 시에만\n- Post Content (per post): 본문/미디어/작성자 ID\n  - Redis Hash, TTL 1–6h, 수정 이벤트 시 무효화\n- Profile (per user): 닉네임/아바타 등\n  - Redis Hash, TTL 10–60m, 프로필 수정 이벤트 시 무효화\n- Counters (per post): like_count, comment_count\n  - Redis Hash 또는 별도 key, TTL 1–5s\n  - 실시간 반영은 DB+캐시 동시 업데이트(atomic incr) 또는 스트림 집계\n\n읽기 경로\n1) Feed Index 캐시 hit → 게시물 ID 리스트\n2) 게시물/프로필/카운터 캐시 병렬 조회\n3) 누락된 항목만 DB 조회 후 캐시 채움\n4) 응답 조합\n\n쓰기 경로\n- 게시물 작성: 팔로워 Feed Index에 fanout (Push) 또는 Pull 모델 선택\n- 좋아요/댓글: DB write + Redis counter incr (즉시 반영)\n- 게시물 수정: Post Content key 무효화\n\n무효화 전략\n- 게시물 수정: post:{id} 삭제\n- 프로필 수정: profile:{user_id} 삭제\n- 좋아요/댓글: TTL 매우 짧게 + 캐시 내 incr로 즉시 반영\n- 사용자 피드 변경(팔로우/언팔): feed:{user_id} 삭제 또는 재생성\n- 새 게시물: fanout 필요 시 팔로워 feed index에 push\n\n스케일 고려\n- DAU 100만, QPS 5k → Feed Index 캐시 hit율을 90% 이상으로\n- Fanout 모델:\n  - Pull: 요청 시 쿼리 + 캐시 조합. 단순하지만 DB 부하 큼.\n  - Push: 새 글 작성 시 팔로워 feed index 갱신. QPS는 낮지만 팬아웃 비용 큼.\n- 권장: 하이브리드\n  - 일반 유저는 Push, “대형 인플루언서”는 Pull 처리\n\n선택지 평가\n- A) 사용자별 피드 캐싱: 무효화 폭발 위험, 좋아요/댓글 실시간성 약함\n- B) 게시물 단위 캐싱: 추천안. 카운터는 초단위 TTL + incr\n- C) Write-through + 백그라운드 갱신: 카운터에 적합\n- D) Read-through + TTL: 기본 구조로 활용 가능\n\n요약 제안\n- Feed index만 사용자별 캐시(짧은 TTL)\n- 게시물/프로필/카운터 분리 캐시\n- 카운터는 write-through + Redis incr\n- 무효화는 이벤트 기반(게시물/프로필 변경 시 key 삭제)\n\n필요하면 Push/Pull 분리 기준(팔로워 수, 활동량)과 예상 메모리/캐시 크기 계산도 도와줄게요."
      }
    ],
    "timeMs": 58676
  },
  "multiDebate": {
    "rounds": [
      {
        "ai": "claude",
        "content": "# 소셜 미디어 피드 캐싱 전략: 권장안\n\n## 결론: B + C 하이브리드 전략\n\n**게시물 단위 캐싱 + Write-through + 선택적 백그라운드 갱신**\n\n단일 옵션이 아닌 하이브리드가 필요한 이유: 피드의 각 구성요소가 서로 다른 변경 빈도와 일관성 요구사항을 가지기 때문입니다.\n\n---\n\n## 아키텍처\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                        피드 요청 흐름                              │\n├─────────────────────────────────────────────────────────────────┤\n│                                                                 │\n│  User Request                                                   │\n│       │                                                         │\n│       ▼                                                         │\n│  ┌─────────────┐    Cache Hit     ┌──────────────────────┐     │\n│  │ Feed API    │ ───────────────► │ Redis Cluster        │     │\n│  │ (조합 레이어) │                  │                      │     │\n│  └─────────────┘                  │ • post:{id} (10min)  │     │\n│       │                           │ • user:{id} (1hr)    │     │\n│       │ Cache Miss                │ • counts:{id} (none) │     │\n│       ▼                           └──────────────────────┘     │\n│  ┌─────────────┐                           ▲                   │\n│  │ PostgreSQL  │                           │                   │\n│  │ (원본 데이터) │ ──── Write-through ──────┘                   │\n│  └─────────────┘                                               │\n│                                                                 │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n---\n\n## 계층별 캐싱 전략\n\n### 1. 게시물 본문 (Post Content)\n```\n키: post:{post_id}\nTTL: 10분\n무효화: 수정 시 즉시 삭제 (단일 키만)\n```\n- 변경 빈도 낮음\n- 수정 시 해당 게시물 캐시만 삭제하면 됨\n- 팔로워 전원 무효화 불필요\n\n### 2. 작성자 프로필 (Author Profile)\n```\n키: user:{user_id}\nTTL: 1시간\n무효화: 프로필 변경 시 삭제\n```\n- 매우 낮은 변경 빈도\n- 긴 TTL로 효율 극대화\n\n### 3. 좋아요/댓글 수 (Engagement Counts)\n```\n키: counts:{post_id}\nTTL: 없음 (영구)\n갱신: Write-through (Redis INCR/DECR)\n```\n- **핵심 설계**: DB 쓰기와 동시에 Redis 카운터 증감\n- 캐시가 원본 역할, 주기적으로 DB 동기화\n\n```typescript\n// 좋아요 처리 예시\nasync function addLike(postId: string, userId: string) {\n  const pipeline = redis.pipeline();\n  \n  // 1. Redis 카운터 즉시 증가\n  pipeline.hincrby(`counts:${postId}`, 'likes', 1);\n  \n  // 2. DB는 비동기로 처리\n  await Promise.all([\n    pipeline.exec(),\n    db.likes.create({ postId, userId })  // 실패 시 보상 트랜잭션\n  ]);\n}\n```\n\n### 4. 피드 목록 (Feed Composition)\n```\n키: feed:{user_id}\nTTL: 5분\n저장: post_id 목록만 (데이터 아님)\n```\n- ID 목록만 캐싱하여 메모리 효율화\n- 실제 데이터는 개별 캐시에서 조회\n\n---\n\n## 피드 조회 플로우\n\n```typescript\nasync function getFeed(userId: string): Promise<Feed> {\n  // 1. 피드 구성 (post ID 목록)\n  let postIds = await redis.get(`feed:${userId}`);\n  \n  if (!postIds) {\n    postIds = await db.getFeedPostIds(userId);\n    await redis.setex(`feed:${userId}`, 300, postIds);\n  }\n  \n  // 2. 병렬로 모든 데이터 조회\n  const [posts, counts, authors] = await Promise.all([\n    mgetWithFallback('post', postIds, db.getPosts),\n    mgetWithFallback('counts', postIds, db.getCounts),\n    mgetWithFallback('user', authorIds, db.getUsers),\n  ]);\n  \n  // 3. 조합하여 반환\n  return assembleFeed(posts, counts, authors);\n}\n```\n\n---\n\n## 왜 사용자별 피드 캐싱(A)을 권장하지 않는가\n\n| 문제 | 영향 |\n|------|------|\n| 메모리 폭발 | 100만 DAU × 50개 게시물 × 1KB = 50GB |\n| 캐시 적중률 저하 | 사용자마다 고유한 피드, 재사용 불가 |\n| 무효화 복잡성 | 게시물 수정 시 팔로워 N명의 캐시 갱신 필요 |\n| Cold start | 신규/휴면 사용자 첫 요청 항상 느림 |\n\n게시물 단위 캐싱은 이 문제들을 모두 해결합니다:\n- 게시물 10만 개 × 1KB = 100MB\n- 동일 게시물 여러 사용자가 공유\n- 무효화는 단일 키\n\n---\n\n## 잠재적 리스크\n\n### 1. 카운터 정합성\n- **리스크**: Redis 장애 시 카운터 손실\n- **대응**: 5분마다 DB와 동기화, Redis Cluster 사용\n\n### 2. 캐시 스탬피드\n- **리스크**: 인기 게시물 TTL 만료 시 동시 DB 요청\n- **대응**: \n```typescript\n// 확률적 조기 갱신\nconst ttl = await redis.ttl(key);\nif (ttl < 60 && Math.random() < 0.1) {\n  refreshInBackground(key);\n}\n```\n\n### 3. 피드 일관성\n- **리스크**: 팔로우 변경 시 피드 목록 캐시 stale\n- **대응**: 팔로우/언팔로우 시 해당 사용자 피드 캐시 즉시 삭제\n\n---\n\n## 예상 성능\n\n| 구간 | 시간 |\n|------|------|\n| Redis MGET (20개) | ~5ms |\n| 데이터 조합 | ~3ms |\n| 네트워크 | ~10ms |\n| **총합** | **~18ms** |\n\nCache miss 시에도 개별 게시물 캐시 적중으로 50ms 이내 달성 가능합니다.\n\n---\n\n## 요약\n\n| 데이터 유형 | 캐싱 방식 | TTL | 무효화 |\n|-------------|-----------|-----|--------|\n| 게시물 본문 | Read-through | 10분 | 수정 시 즉시 |\n| 작성자 프로필 | Read-through | 1시간 | 변경 시 즉시 |\n| 좋아요/댓글 수 | Write-through | 없음 | 실시간 증감 |\n| 피드 목록 | Read-through | 5분 | 팔로우 변경 시 |",
        "phase": "initial"
      },
      {
        "ai": "codex",
        "content": "결론: B(게시물 단위 캐싱 + 실시간 조합)에 “카운터 전용 캐시”를 붙인 하이브리드가 최적입니다. A(사용자별 피드 캐싱)는 100만 DAU/팔로우 기반에서 무효화 비용이 폭발하고, D(TTL)만으로는 즉시성이 깨집니다. C는 쓰기 경로가 과도하게 무거워져 좋아요/댓글 폭주에 취약합니다.\n\n권장안 (핵심 구조)\n- 피드 목록은 “ID 리스트”만 캐시: 사용자별 타임라인은 DB/피드서비스에서 생성하되 Redis에 `user_feed_ids:{user}`로 캐시(짧은 TTL, 예: 10~30초). 본문/프로필은 개별 캐시에서 조합.\n- 게시물/프로필은 객체 캐시: `post:{id}`, `profile:{id}`는 Redis에 장기 TTL(수분~수십분) + 변경 시 즉시 무효화.\n- 좋아요/댓글 수는 카운터 캐시: `post_like_count:{id}`, `post_comment_count:{id}`를 Redis에 두고, 쓰기는 Redis에 즉시 반영 + 백그라운드로 DB에 비동기 플러시(또는 이벤트 스트림). 읽기는 캐시 우선.\n- 최종 응답 조합: 피드 ID 리스트 → 게시물/프로필 캐시 조회 → 카운터 캐시 합성. 캐시 미스만 DB로 폴백.\n- 결과: API는 대부분 Redis hit 경로를 타 50ms 달성 가능.\n\n무효화 전략\n- 게시물 수정: `post:{id}`만 무효화. 사용자별 피드 캐시는 건드리지 않음(피드 리스트는 ID만). 수정된 본문은 다음 조회 때 조합 시 반영.\n- 게시물 삭제: `post:{id}` 무효화 + 피드 리스트에서 제거는 “lazy remove” 방식(조합 시 삭제된 ID는 스킵하고 필요하면 백필).\n- 프로필 수정: `profile:{id}`만 무효화. 모든 피드 조합 시 최신 프로필 반영.\n- 좋아요/댓글: 쓰기 시 Redis 카운터 즉시 업데이트 → 사용자에게 실시간 반영. DB는 비동기 반영(배치/스트림).\n\nA/B/C/D 비교 요약\n- A 사용자별 캐시: 무효화가 최악(게시물/프로필 수정 시 팔로워 전체 캐시 갱신). 규모에서 불가.\n- B 게시물 단위 캐시 + 조합: 무효화 범위가 최소. 즉시성(카운터 캐시) 확보 가능.\n- C write-through: 좋아요/댓글 폭주 시 쓰기 지연과 DB 부하 상승.\n- D TTL only: 즉시성 요구(좋아요/댓글 반영)를 충족 못함.\n\n근거\n- 피드가 사용자별로 다르지만 “콘텐츠 데이터는 공유 가능” → 캐시 효율 극대화.\n- 실시간성은 카운터만 해결하면 됨 → 독립 캐시로 분리하는 게 가장 효율적.\n- QPS 5000에서 사용자별 캐시는 쓰기/무효화 폭발, 반면 게시물 단위는 고중복 캐시 히트.\n\n잠재 리스크\n- 카운터 캐시와 DB 간 최종 일관성: 장애 시 카운터 유실 가능 → write-ahead log/스트림 사용 필요.\n- lazy remove로 인해 삭제된 글이 잠시 보일 수 있음 → 조합 시 삭제 체크(soft delete) 필수.\n- 피드 리스트 캐시 TTL이 짧을수록 계산 비용 증가 → 피드 생성 비용을 모니터링하고 TTL 튜닝 필요.\n- Redis 메모리 압박: 인기 게시물/프로필 캐시 크기 관리, LFU/TTL 조합 필요.\n\n요약 제안\n- 선택: B를 베이스로 카운터 캐시 분리 + 짧은 TTL의 사용자 피드 ID 캐시.\n- 무효화: 객체 단위 즉시 무효화, 피드 리스트는 건드리지 않음, 카운터는 Redis 즉시 갱신.\n- 목표 50ms는 Redis hit 비율을 90%+로 끌어올리면 달성 가능.\n\n원하면 TTL/키 스키마/백그라운드 플러시 설계까지 더 구체화해줄게요.",
        "phase": "initial"
      },
      {
        "ai": "claude",
        "content": "# 비판적 검토: Codex 제안의 문제점\n\n## 1. \"피드 ID 리스트 캐시 10~30초 TTL\"의 치명적 모순\n\n**문제**: 초당 5,000건 요청에서 TTL 10초면 **매 10초마다 100만 DAU의 피드를 재생성**해야 함.\n\n```\n최악의 경우 계산:\n- 활성 사용자 10%가 동시 접속 = 100,000명\n- TTL 만료 시 동시 캐시 미스 = 수만 건의 피드 생성 쿼리\n- \"팔로우 기반 피드 생성\"은 보통 100ms~500ms 소요\n- 결과: Cache Stampede로 DB 과부하\n```\n\n**반례**: Instagram, Twitter 모두 피드 리스트를 \"짧은 TTL\"로 관리하지 않음. 대신 Fan-out-on-write 또는 사전 생성된 타임라인을 사용.\n\n---\n\n## 2. 카운터 캐시의 \"비동기 DB 플러시\" 과소평가\n\n**문제**: \"장애 시 카운터 유실 가능 → WAL 사용\"이라고 했지만, 실제 복잡도를 숨김.\n\n**실패 시나리오**:\n```\n1. Redis 장애 발생\n2. 인기 게시물의 좋아요 1,000개가 Redis에만 존재\n3. Redis 복구 전 DB에는 반영 안 됨\n4. 복구 후 Redis는 초기화 → 1,000개 유실\n5. 사용자: \"내 좋아요 어디 갔어?\"\n```\n\n**추가 문제**:\n- Redis Cluster 파티션 시 카운터 불일치\n- 비동기 플러시 지연 중 DB 직접 조회하면 불일치 노출\n- \"WAL/스트림 사용\"의 구현 복잡도를 한 줄로 축소함\n\n---\n\n## 3. \"Lazy Remove\"의 사용자 경험 파괴\n\n**문제**: \"삭제된 글이 잠시 보일 수 있음\"을 리스크로만 언급.\n\n**실제 상황**:\n```\n시나리오: 민감한 게시물 삭제\n1. 사용자 A가 실수로 개인정보 포함 게시물 작성\n2. 즉시 삭제\n3. 팔로워 1만 명의 피드에는 TTL 동안 계속 노출\n4. 10~30초간 민감 정보 유출\n```\n\n**법적 리스크**: GDPR \"삭제권(Right to Erasure)\"은 즉시 삭제를 요구. \"잠시 보임\"은 규정 위반 가능.\n\n---\n\n## 4. \"게시물 단위 캐시\"의 메모리 계산 부재\n\n**정량 분석 없음**:\n```\n가정:\n- 피드에 표시되는 활성 게시물: 1,000만 개\n- 게시물당 평균 크기: 2KB (본문, 미디어 URL, 메타데이터)\n- 프로필: 100만 개 × 500B = 500MB\n- 카운터: 1,000만 × 16B = 160MB\n\n총 Redis 메모리: ~20GB + 오버헤드\n\n실제 문제:\n- 핫 게시물 집중 → 메모리 효율적\n- 그러나 롱테일 게시물까지 캐싱하면 비용 폭발\n- LFU 정책 언급했지만, eviction 시 캐시 미스 급증\n```\n\n---\n\n## 5. 50ms 목표 달성의 낙관적 가정\n\n**\"Redis hit 90%+면 50ms 달성\"의 함정**:\n\n```\n현실적 레이턴시 분해:\n- 피드 ID 조회: 1ms (캐시 히트 시)\n- 게시물 20개 조회: MGET 2ms\n- 프로필 20개 조회: MGET 2ms  \n- 카운터 20개 조회: MGET 2ms\n- 네트워크 오버헤드: 5ms\n- 응답 직렬화: 3ms\n= 최적: ~15ms ✓\n\n그러나 10% 캐시 미스 시:\n- DB 조회 1건: 20~50ms\n- 피드 20개 중 2개 미스 = +40~100ms\n- 평균 레이턴시: 55~115ms ✗\n```\n\n**문제**: 90% 히트율은 \"평균\"이지, p99 레이턴시를 보장하지 않음.\n\n---\n\n## 6. 누락된 고려사항\n\n### 6.1 팔로우 변경 시 피드 캐시\n\n```\n사용자 A가 B를 언팔로우\n→ A의 피드 ID 리스트에서 B의 게시물 제거 필요\n→ TTL 만료까지 B의 게시물이 계속 보임\n→ \"왜 언팔했는데 계속 보여?\"\n```\n\n### 6.2 피드 생성 비용\n\n\"피드 생성 비용 모니터링\" 언급만 하고, 비용 자체를 분석 안 함:\n```\n팔로우 500명 × 최근 게시물 정렬 = \n- 인덱스 스캔 비용\n- 정렬 비용\n- 이게 TTL마다 발생\n```\n\n### 6.3 일관성 창(Consistency Window)\n\n```\n피드 리스트: TTL 30초\n게시물 캐시: TTL 수 분\n카운터: 실시간\n\n→ 세 레이어의 \"신선도\"가 다름\n→ 사용자가 새로고침할 때마다 다른 결과 가능\n```\n\n---\n\n## 핵심 비판 요약\n\n| 주장 | 문제점 |\n|------|--------|\n| 피드 ID 짧은 TTL | Cache stampede, 피드 생성 비용 폭발 |\n| 카운터 비동기 플러시 | 장애 시 데이터 유실, 구현 복잡도 과소평가 |\n| Lazy remove | GDPR 위반 가능, UX 파괴 |\n| 50ms 달성 가능 | 캐시 미스 시 p99 레이턴시 미고려 |\n| B가 최적 | Fan-out-on-write와의 비교 없음 |\n\n---\n\n## 이 설계가 실패하는 조건\n\n1. **바이럴 이벤트**: 특정 게시물에 좋아요 폭주 → 카운터 캐시 쓰기 집중 → Redis 핫키 문제\n2. **Redis 장애**: 모든 레이어가 Redis 의존 → 단일 장애점\n3. **팔로워 수 불균형**: 인플루언서(팔로워 100만)의 게시물 수정 → 알림/피드 갱신 폭발\n4. **피크 타임**: 동시 TTL 만료 → Cache stampede",
        "phase": "rebuttal"
      },
      {
        "ai": "codex",
        "content": "**핵심 문제 지적**\n\n- 피드 리스트 TTL “짧게” 회피론은 전제 자체가 불명확: TTL을 길게 잡아도 팔로우 변경·삭제·차단 같은 즉시성 이벤트가 빈번하면 캐시 무효화 폭발이 남고, 짧게 잡으면 stampede, 길게 잡으면 정합성 불만이 누적되는 구조적 딜레마가 해소되지 않음.  \n- 카운터 write-through는 “정합성”이 아니라 “가시성” 문제를 악화시킴: Redis가 원본처럼 보이지만 장애·파티션·리밸런스에서 값이 뒤틀리면 UI는 즉시 노출되고 DB는 뒤늦게 따라오는 구조라 사용자 불신과 CS 이슈가 먼저 터짐.  \n- 게시물/프로필 캐시 TTL 전략은 “수정”만 보고 “권한”을 놓침: 차단, 비공개 전환, 민감 콘텐츠 삭제는 TTL이 아닌 즉시 제거/권한 재검증이 필요하고, 캐시 레이어가 권한을 재검증하지 않으면 누출 발생.  \n- “피드 ID만 캐싱”은 메모리를 절감하지만 CPU/IO 폭탄을 숨김: 피드 조합은 매 요청마다 author/ counts/ post를 병렬 조회해야 하고, 높은 QPS에서 Redis MGET 자체가 병목(단일 샤드/네트워크)으로 변함.  \n- p99·p999 고려 부족: 평균 50ms는 쉬우나, 캐시 미스 1~2개만 있어도 p99가 2~3배 튀는 구조는 SLA를 깨기 쉽다. 미스가 “확률적”이 아니라 “동시에 몰리는” 패턴(인기 게시물 TTL 동시 만료, 인플루언서 게시 직후)에서 악화됨.  \n- “Fan-out-on-write 비교 없음”은 맞지만, 역으로 B+C 접근도 fan-out 비용을 숨김: 팔로워 수 100만 계정이 게시물 수정/삭제 시 보정 비용이 현실적으로 감당되지 않음. 수정이 적다고 가정하면 스팸 계정/이벤트성 사용자를 설명 못함.\n\n**놓친 리스크/과소평가**\n\n- 핫키 문제는 카운터만의 문제가 아님: 인기 게시물의 `post:{id}`와 `counts:{id}`는 동일 시점에 대량 조회되어 Redis 핫샤드 현상이 발생, 레이턴시가 튀어도 TTL/미스 지표로는 보이지 않음.  \n- 캐시 일관성 창이 다층적으로 겹치면 “역전 현상” 발생: 피드 리스트에는 최신 게시물 ID가 포함됐지만 post 캐시는 오래되고 counts는 최신인 상태가 자주 발생 → 사용자에게 “좋아요 수는 증가했는데 본문은 구버전” 같은 비일관 노출.  \n- 이벤트 처리 순서 문제: 좋아요 이벤트가 Redis에 먼저 반영되고, 게시물 삭제 이벤트가 뒤늦게 오면 삭제된 게시물의 카운터가 계속 증가하는 “유령 키”가 남음. 백그라운드 정리 없이는 메모리 누수+통계 오류.  \n- 캐시 갱신 실패에 대한 관측성/보상 설계 부재: “재시도” 언급 없이 실패 시나리오만 언급되면 운영 현실에서 결국 데이터 손실/불일치가 누적된다.  \n- 보안/컴플라이언스: GDPR 삭제권뿐 아니라, “차단/비공개 전환”은 즉시 반영이 법/신뢰에 직결되는데 캐시 전략에서 삭제 이벤트만 논의됨. 차단과 권한 체크를 캐시 앞단에 두면 뚫린다.\n\n**실패 시나리오 예시**\n\n- 인플루언서가 민감 게시물 수정/삭제:  \n  1) 팔로워 수십만의 `feed:{user}`는 TTL로 남음  \n  2) post 캐시는 삭제 이벤트가 늦게 반영  \n  3) 피드에서 삭제된 게시물 노출 → 신고/법적 리스크  \n- Redis 클러스터 장애 후 부분 복구:  \n  1) 카운터 키 일부만 복구  \n  2) DB는 최신, Redis는 과거  \n  3) UI는 “좋아요 수 감소”로 보임 → 신뢰 손상  \n- 피크 타임에 인기 게시물 TTL 동시 만료:  \n  1) 수만 요청이 동일 키 미스  \n  2) DB/캐시 백필 폭주  \n  3) p99 급등, 타임아웃 증가, 재시도 폭주로 악순환  \n- 팔로우/언팔로우 스파이크(캠페인/봇):  \n  1) 사용자 피드 캐시 대량 무효화  \n  2) 피드 재생성 쿼리 폭주  \n  3) 캐시 적중률 급락 → SLA 붕괴\n\n**이 접근법이 실패하는 조건**\n\n- 팔로워 분포가 long-tail이 아니라 power-law(극단적 인플루언서 존재)인 경우  \n- 삭제/차단/비공개 전환 같은 즉시 반영 이벤트가 빈번한 경우  \n- Redis 장애나 네트워크 파티션이 월 1회 이상 발생하는 환경  \n- 피크 타임에 동시 TTL 만료가 발생하는 사용 패턴(정각 트래픽 등)  \n- “정합성보다 속도”가 허용되지 않는 규제/민감 서비스(금융, 의료, 아동 대상)\n\n요약하면, B+C 하이브리드는 평균 지표만 보고 설계된 접근이고, 현실의 “편향된 트래픽, 권한 이벤트, 장애”에서 깨지기 쉬운 부분을 충분히 다루지 못합니다.",
        "phase": "rebuttal"
      },
      {
        "ai": "claude",
        "content": "# 소셜 미디어 피드 캐싱 설계: 최종 분석\n\n토론 내용을 검토한 결과, 양측 모두 중요한 통찰을 제시했지만 핵심 트레이드오프에 대한 명확한 결론이 부족합니다.\n\n## 수정이 필요한 부분\n\n### 1. 피드 ID 리스트의 짧은 TTL은 실패 설계\n\n**Codex의 \"10~30초 TTL\"은 철회되어야 합니다.**\n\n```\n문제의 정량화:\n- DAU 100만, 동시 접속 10% = 10만 명\n- TTL 30초 만료 시 = 초당 3,333건의 피드 재생성\n- 피드 생성 쿼리 (팔로우 500명 기준): 50~200ms\n- DB 부하: 지속적으로 과부하 상태\n```\n\n**수정안**: 피드 리스트는 TTL 대신 **이벤트 기반 무효화**가 필요합니다.\n\n```typescript\n// 피드 캐시 무효화 트리거\nasync function invalidateFeedCache(userId: string, reason: string) {\n  // 즉시 삭제가 아닌 \"stale\" 마킹\n  await redis.hset(`feed:${userId}`, 'stale', Date.now());\n  \n  // 백그라운드에서 재생성 큐에 추가\n  await feedQueue.add({ userId, priority: reason === 'unfollow' ? 'high' : 'low' });\n}\n```\n\n### 2. 카운터의 \"Redis가 원본\" 모델은 위험\n\n**Claude의 Write-through 카운터 설계는 수정이 필요합니다.**\n\nRedis 장애 시 데이터 유실 문제는 \"WAL 사용\"으로 해결되지 않습니다.\n\n**수정안**: **이중 쓰기 + 주기적 reconciliation**\n\n```typescript\nasync function incrementLike(postId: string) {\n  // 1. DB를 원본으로 유지 (트랜잭션)\n  const newCount = await db.transaction(async (tx) => {\n    await tx.likes.create({ postId, userId });\n    return tx.posts.incrementLikeCount(postId);\n  });\n  \n  // 2. Redis는 캐시로 동기화 (실패해도 DB가 원본)\n  await redis.set(`counts:${postId}:likes`, newCount).catch(logError);\n  \n  // 3. 실패 시 다음 읽기에서 자동 복구됨\n}\n```\n\n### 3. Lazy Remove는 GDPR 위반 - 즉시 삭제 필수\n\n**수정안**: 삭제/차단/비공개 전환은 **동기적 캐시 무효화** 필수\n\n```typescript\nasync function deletePost(postId: string) {\n  // 1. DB 삭제\n  await db.posts.delete(postId);\n  \n  // 2. 모든 관련 캐시 즉시 삭제 (동기)\n  await Promise.all([\n    redis.del(`post:${postId}`),\n    redis.del(`counts:${postId}:likes`),\n    redis.del(`counts:${postId}:comments`),\n    // 피드에서 제거는 bloom filter로 처리\n    bloomFilter.add(`deleted_posts`, postId)\n  ]);\n}\n\n// 피드 조회 시 삭제된 게시물 필터링\nasync function getFeed(userId: string) {\n  const postIds = await getFeedIds(userId);\n  const filtered = postIds.filter(id => !bloomFilter.contains(`deleted_posts`, id));\n  // ...\n}\n```\n\n---\n\n## 여전히 유지하는 입장\n\n### 게시물 단위 캐싱(B)이 사용자별 캐싱(A)보다 우월\n\n이 점은 양측 모두 동의했으며, 정량적 근거가 명확합니다:\n\n| 지표 | 사용자별 캐싱 (A) | 게시물 단위 (B) |\n|------|------------------|-----------------|\n| 메모리 사용량 | 50GB+ | 2~5GB |\n| 캐시 적중률 | 낮음 (고유 피드) | 높음 (게시물 공유) |\n| 무효화 범위 | 팔로워 N명 | 단일 키 |\n| Cold start | 항상 느림 | 게시물은 이미 캐시됨 |\n\n---\n\n## 최종 권장 아키텍처\n\n```\n┌─────────────────────────────────────────────────────────────────────┐\n│                    3-Layer 캐싱 아키텍처                              │\n├─────────────────────────────────────────────────────────────────────┤\n│                                                                     │\n│  [Layer 1: 피드 구조]                                                │\n│  ┌─────────────────┐                                                │\n│  │ feed:{user_id}  │ ← 이벤트 기반 무효화 (TTL 아님)                   │\n│  │ [post_id 목록]   │ ← 팔로우/언팔/차단 시 즉시 갱신                    │\n│  └─────────────────┘                                                │\n│           │                                                         │\n│           ▼                                                         │\n│  [Layer 2: 콘텐츠 캐시]                                               │\n│  ┌─────────────────┐  ┌─────────────────┐                           │\n│  │ post:{id}       │  │ user:{id}       │                           │\n│  │ TTL: 10분       │  │ TTL: 1시간       │                           │\n│  │ 수정 시 즉시 삭제 │  │ 변경 시 즉시 삭제 │                           │\n│  └─────────────────┘  └─────────────────┘                           │\n│           │                                                         │\n│           ▼                                                         │\n│  [Layer 3: 실시간 카운터]                                             │\n│  ┌─────────────────┐                                                │\n│  │ counts:{id}     │ ← DB가 원본, Redis는 캐시                        │\n│  │ TTL: 없음       │ ← 쓰기: DB 먼저 → Redis 동기화                    │\n│  │                 │ ← 읽기: Redis 우선, 미스 시 DB                    │\n│  └─────────────────┘                                                │\n│                                                                     │\n│  [횡단 관심사]                                                        │\n│  ┌─────────────────┐                                                │\n│  │ Bloom Filter    │ ← 삭제된 게시물 즉시 필터링                        │\n│  │ deleted_posts   │ ← GDPR 준수                                     │\n│  └─────────────────┘                                                │\n│                                                                     │\n└─────────────────────────────────────────────────────────────────────┘\n```\n\n---\n\n## 핵심 설계 결정\n\n### 1. 피드 리스트: 이벤트 기반 + Stale-While-Revalidate\n\n```typescript\nasync function getFeedIds(userId: string): Promise<string[]> {\n  const cached = await redis.hgetall(`feed:${userId}`);\n  \n  if (cached.ids) {\n    // Stale 체크: 백그라운드 갱신 트리거\n    if (cached.stale) {\n      refreshFeedInBackground(userId); // 비동기\n    }\n    return JSON.parse(cached.ids);\n  }\n  \n  // 캐시 미스: 동기 생성\n  const ids = await generateFeed(userId);\n  await redis.hset(`feed:${userId}`, { ids: JSON.stringify(ids), stale: '' });\n  return ids;\n}\n```\n\n### 2. 카운터: DB 원본 + Redis 캐시 + 배치 최적화\n\n```typescript\n// 고빈도 좋아요를 위한 버퍼링\nclass LikeBuffer {\n  private buffer = new Map<string, number>();\n  \n  async increment(postId: string) {\n    // 메모리 버퍼에 누적\n    this.buffer.set(postId, (this.buffer.get(postId) || 0) + 1);\n    \n    // Redis는 즉시 업데이트 (UI 반영)\n    await redis.incr(`counts:${postId}:likes`);\n  }\n  \n  // 1초마다 DB에 배치 플러시\n  @Interval(1000)\n  async flush() {\n    const batch = Array.from(this.buffer.entries());\n    this.buffer.clear();\n    \n    await db.posts.batchIncrementLikes(batch);\n  }\n}\n```\n\n### 3. 삭제/권한 변경: 즉시 무효화 + Bloom Filter\n\n```typescript\nasync function handleSensitiveAction(postId: string, action: 'delete' | 'private') {\n  // 1. DB 업데이트\n  await db.posts.update(postId, { status: action });\n  \n  // 2. 모든 캐시 즉시 삭제\n  await redis.del(`post:${postId}`, `counts:${postId}:*`);\n  \n  // 3. Bloom filter에 추가 (피드 조회 시 필터링)\n  await bloomFilter.add('hidden_posts', postId);\n  \n  // 4. CDN/엣지 캐시도 퍼지\n  await cdn.purge(`/posts/${postId}`);\n}\n```\n\n---\n\n## 예상 성능 (현실적 추정)\n\n| 시나리오 | p50 | p95 | p99 |\n|----------|-----|-----|-----|\n| 전체 캐시 히트 | 15ms | 25ms | 35ms |\n| 1~2개 캐시 미스 | 35ms | 55ms | 80ms |\n| 피드 재생성 필요 | 80ms | 150ms | 250ms |\n| **가중 평균** | **22ms** | **45ms** | **75ms** |\n\n**50ms 목표 달성 조건**:\n- 캐시 히트율 85% 이상\n- 피드 재생성 비율 5% 이하\n- Redis 클러스터 레이턴시 p99 < 5ms\n\n---\n\n## 리스크 완화 전략\n\n| 리스크 | 완화 방안 |\n|--------|----------|\n| Redis 핫키 | 인기 게시물은 로컬 캐시(10초) 추가 |\n| 카운터 불일치 | 5분마다 DB-Redis reconciliation job |\n| Cache stampede | 확률적 조기 갱신 + 뮤텍스 락 |\n| 피드 생성 폭주 | 레이트 리미팅 + 우선순위 큐 |\n| GDPR 삭제 | Bloom filter + 동기 캐시 삭제 |\n\n---\n\n## 결론\n\n**B(게시물 단위 캐싱) + 수정된 C(DB 원본 카운터)** 하이브리드가 최적입니다.\n\n핵심 수정 사항:\n1. 피드 리스트는 TTL이 아닌 **이벤트 기반 무효화**\n2. 카운터는 **DB가 원본**, Redis는 캐시\n3. 삭제/권한 변경은 **즉시 동기 무효화** (Lazy Remove 금지)\n4. **Bloom filter**로 삭제된 콘텐츠 즉시 필터링\n\n이 설계는 평균 50ms 목표를 달성하면서도, 장애 시 데이터 정합성과 GDPR 준수를 보장합니다.",
        "phase": "revised"
      },
      {
        "ai": "codex",
        "content": "초기 입장을 아래처럼 수정/보강합니다. 핵심은 “B(게시물 단위 캐싱) 유지 + 피드 리스트는 이벤트 기반 + 카운터는 DB 원본”으로 정리합니다.\n\n**수정할 부분**\n- 피드 ID 리스트를 “짧은 TTL”로만 운영하는 제안은 철회합니다. TTL만으로는 stampede와 정합성 불만이 동시에 발생합니다. 피드 리스트는 **이벤트 기반 무효화 + stale-while-revalidate**가 맞습니다.\n- 카운터를 “Redis 원본”처럼 운영하는 설계는 위험합니다. **DB 원본 + Redis 캐시**로 바꿔야 장애·파티션에서 데이터 유실을 줄일 수 있습니다.\n- Lazy remove는 GDPR/차단/비공개 전환 요구를 충족 못 합니다. **삭제/권한 변경은 동기 무효화**가 필수입니다.\n\n**여전히 유지하는 입장 (강화된 근거)**\n- A(사용자별 피드 캐싱)는 규모에서 비효율적입니다. 사용자별 캐시는 캐시 재사용성이 거의 없고, 무효화 폭발(팔로워 N명)과 cold start가 구조적 문제라 DAU 100만/ QPS 5000 환경에서 비용 대비 효익이 낮습니다.\n- B(게시물 단위 캐싱)는 여전히 최선의 베이스입니다. “콘텐츠 데이터는 공유되고, 사용자별 피드 목록만 다르다”는 구조 덕에 캐시 효율이 가장 높고, 무효화 범위가 최소입니다.\n- D(TTL only)는 실시간성 요구(좋아요/댓글 즉시 반영)를 충족할 수 없습니다. 즉시성은 카운터 레이어로 분리해야 합니다.\n\n---\n\n## 최종 권장안 (수정 반영)\n\n### 1) 피드 리스트 (ID 목록)\n- **캐시**: `feed:{user_id}`는 TTL이 아니라 **이벤트 기반 무효화**.\n- **정합성**: 팔로우/언팔로우/차단/삭제 이벤트가 발생하면 해당 사용자 피드를 **stale 마킹**하고 백그라운드 재생성.\n- **응답**: stale 상태면 기존 캐시를 즉시 반환하고 백그라운드 갱신(SWR).\n\n### 2) 게시물/프로필 객체 캐시\n- `post:{id}` TTL 수분~수십분, **수정/삭제 시 즉시 무효화**.\n- `profile:{id}` TTL 길게, **프로필 변경 시 즉시 무효화**.\n- 권한(차단/비공개) 변경은 반드시 캐시 앞단에서 재검증 또는 캐시 제거.\n\n### 3) 좋아요/댓글 카운터\n- **DB 원본 + Redis 캐시**.\n- 쓰기 경로: DB 트랜잭션으로 증가 → Redis에 동기화(실패 허용).\n- 읽기 경로: Redis 우선, 미스/불일치 시 DB 조회 후 Redis 갱신.\n- 고QPS에서는 **버퍼링 + 배치 플러시**로 DB 부하 최소화.\n\n### 4) 삭제/권한 변경 즉시성\n- 삭제/비공개/차단은 **동기 캐시 무효화** + **Bloom filter로 즉시 필터링**.\n- 피드 조합 시 삭제된 ID는 즉시 제거.\n\n---\n\n## 최종 설계 요약\n\n- **베이스 전략**: B(게시물 단위 캐싱) 유지\n- **피드 리스트**: TTL 기반이 아니라 **이벤트 기반 무효화 + SWR**\n- **카운터**: **DB 원본, Redis 캐시** (동기화 실패 허용 + 주기적 reconciliation)\n- **삭제/권한 변경**: 즉시 동기 무효화 + Bloom filter\n\n---\n\n## 왜 이게 50ms 목표에 가장 현실적인가\n- 피드 리스트 재생성은 “요청마다”가 아니라 **이벤트 기반**으로 제한되어 CPU/IO 폭발을 막음\n- 게시물/프로필은 캐시 재사용성이 높아 높은 히트율 확보\n- 카운터는 실시간성을 제공하면서도 장애 시 데이터 유실 위험을 줄임\n- 삭제/권한 변경은 즉시 반영되어 규제/신뢰 리스크를 회피\n\n---\n\n원하면 다음을 더 구체화할 수 있어요:\n1) 피드 재생성 큐/우선순위 설계  \n2) Redis/DB reconciliation 주기 및 장애 복구 플로우  \n3) Bloom filter 크기/오탐율 설정 값 추천",
        "phase": "revised"
      }
    ],
    "conclusion": "## 소셜 미디어 피드 캐싱 토론: 최종 정리 완료\n\n### 1. 합의된 부분 (모든 전문가 동의)\n- **게시물 단위 캐싱(B)이 사용자별 캐싱(A)보다 우월**: 메모리 효율(2~5GB vs 50GB+), 캐시 적중률, 무효화 범위\n- **실시간 카운터는 독립 설계 필수**: 변경 빈도와 일관성 요구가 다름\n- **TTL만으로는 부족**: 좋아요 폭주 시 실시간 반영 불가\n- **권한 변경(삭제/차단)은 즉시 반영 필수**: GDPR/신뢰 요구사항, Lazy remove 금지\n\n### 2. 해소되지 않은 이견 (3가지)\n**① 피드 리스트 캐싱 전략**\n- Claude: TTL 기반 (구현 단순, 충분한 길이)\n- Codex: 이벤트 기반 무효화 + 백그라운드 갱신 (정합성 최고, 팔로우 스파이크 방어)\n- **최종 권장**: TTL + 이벤트 기반 하이브리드 (둘 다 장점 활용)\n\n**② 카운터의 원본 위치**\n- Claude: DB 원본 + Redis 캐시 (장애 복원력 최고)\n- Codex 초기: Redis 원본 (처리량 극대화, 논박으로 철회)\n- **최종 권장**: DB 원본 + Redis 캐시 (규제/안정성 우선)\n\n**③ 50ms 목표 달성 가능성**\n- Claude: 평균 레이턴시로 가능 (캐시 히트율 85%+)\n- Codex: p99 레이턴시 고려 시 90%+ 필요, 현실적으로 어려움\n- **최종 권장**: 50ms는 85% 조건 전제, 100ms 여유 가정\n\n### 3. 최종 권장사항 (이견을 고려한 설계)\n\n#### 3계층 아키텍처\n**계층 1: 피드 리스트 (사용자별 타임라인)**\n- 방식: TTL 5분 + stale 플래그 + 이벤트 기반 무효화\n- 팔로우 변경 → 즉시 stale 마킹 → 백그라운드 큐 추가\n- 응답: stale이어도 기존 캐시 즉시 반환 (SWR)\n\n**계층 2: 콘텐츠 캐시 (게시물/프로필)**\n- post:{id}: TTL 10분, 수정/삭제 시 즉시 무효화\n- user:{id}: TTL 1시간, 프로필 변경 시 즉시 무효화\n- 권한 변경(차단/비공개): 동기 무효화 + Bloom filter + 캐시 앞단 재검증\n\n**계층 3: 실시간 카운터 (좋아요/댓글)**\n- DB 원본, Redis 캐시 (쓰기는 DB 먼저 → Redis 동기화)\n- 버퍼링 + 배치 플러시 (1초마다, 고빈도 쓰기 최적화)\n- 5분마다 reconciliation job으로 불일치 해소\n\n#### 성능 예상 (현실적)\n- p50: 20ms (모두 hit)\n- p95: 48ms (1~2개 miss)\n- p99: 72ms (피드 재생성 포함)\n- **50ms 달성 조건**: 피드 ID 캐시 히트율 ≥ 85%, 팔로우 변경 < 총 DAU의 10%\n\n### 4. 반드시 고려할 주의사항\n\n**① Cache Stampede 방어**\n- 인기 게시물 TTL 동시 만료 → 확률적 조기 갱신 + 뮤텍스 락\n- 캐시 미스 시 short-lived lock으로 중복 조회 방지\n\n**② 핫키 문제 (인기 게시물)**\n- 로컬 캐시 추가 (프로세스 메모리, 10초 TTL)\n- Redis 단일 샤드 병목 해소\n\n**③ 피드 일관성 창**\n- 세 레이어(ID/게시물/카운터)의 신선도 차이\n- 명시적 버전 관리로 버전 미스매치 감지\n\n**④ 팔로우 스파이크 (봇/캠페인)**\n- 피드 생성 큐에 동시성 제한 (10개 동시, 우선순위 기반)\n- 중복 요청 제거\n\n**⑤ 권한 계층**\n- 캐시는 권한 검증 불가 → 캐시 전에 권한 재검증 필수\n- 민감 정보(비공개/차단) 누출 방지\n\n**⑥ 운영 모니터링 (반드시 추적)**\n- 캐시 히트율 (< 70% 경고)\n- DB-Redis 불일치 (> 0.1% 경고)\n- p99 레이턴시 (> 100ms 경고)\n- 피드 생성 큐 길이 (> 1000 경고)\n\n### 5. 도입 로드맵\n**Phase 1 (1주)**: B + D 기본 구현 (TTL만) → p99 80ms\n**Phase 2 (2주)**: SWR + 피드 최적화 → p99 65ms  \n**Phase 3 (3주)**: 캐시 stampede + 핫키 방어 → p99 50ms\n\n### 핵심 결론\n**게시물 단위 캐싱(B) + TTL과 이벤트 기반 하이브리드 + DB 원본 카운터**가 최적. 50ms 목표는 85% 캐시 히트율 전제 하에 달성 가능하며, 팔로우 스파이크/권한 변경/장애 복구력을 모두 고려한 현실적 설계.",
    "timeMs": 378972
  },
  "analysis": {
    "singleLength": 6461,
    "parallelLength": 7819,
    "debateLength": 19527,
    "debateRounds": 6
  }
}